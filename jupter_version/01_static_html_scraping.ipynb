{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "toc_visible": true
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# üìö Module 1: Static HTML Scraping\n",
                "\n",
                "**Learn web scraping fundamentals using BeautifulSoup**\n",
                "\n",
                "In this notebook, you'll learn:\n",
                "- How to fetch HTML using `requests`\n",
                "- Parse HTML with BeautifulSoup\n",
                "- Extract structured data\n",
                "- Validate data with Pydantic\n",
                "\n",
                "**Target Website**: [Bonbanh.com](https://bonbanh.com) (Vietnamese car marketplace)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üîß Setup\n",
                "Install required packages (run once)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "!pip install requests beautifulsoup4 pydantic -q\n",
                "print(\"‚úÖ Packages installed!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## Step 1: Basic HTTP Request\n",
                "\n",
                "**Goal**: Fetch HTML content from a website\n",
                "\n",
                "**Concepts**:\n",
                "- HTTP GET request\n",
                "- Response object\n",
                "- Basic HTML structure"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import requests\n",
                "\n",
                "# Target URL - page 1 of car listings\n",
                "url = \"https://bonbanh.com/oto/page,1?q=\"\n",
                "\n",
                "print(\"Fetching webpage...\")\n",
                "print(f\"URL: {url}\\n\")\n",
                "\n",
                "# Send GET request\n",
                "response = requests.get(url)\n",
                "\n",
                "# Check if request was successful\n",
                "print(f\"Status Code: {response.status_code}\")  # 200 means success\n",
                "print(f\"Content Type: {response.headers['content-type']}\\n\")\n",
                "\n",
                "# Print first 500 characters of HTML\n",
                "print(\"=\" * 50)\n",
                "print(\"HTML PREVIEW (first 500 characters):\")\n",
                "print(\"=\" * 50)\n",
                "print(response.text[:500])\n",
                "print(\"...\")\n",
                "\n",
                "# Full HTML is available in response.text\n",
                "print(f\"\\nTotal HTML length: {len(response.text)} characters\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### üí° Key Takeaways\n",
                "\n",
                "- `requests.get(url)` sends an HTTP GET request\n",
                "- `response.status_code` = 200 means success\n",
                "- `response.text` contains the raw HTML"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## Step 2: Parse HTML with BeautifulSoup\n",
                "\n",
                "**Goal**: Parse HTML and extract car titles using CSS selectors\n",
                "\n",
                "**Concepts**:\n",
                "- BeautifulSoup HTML parser\n",
                "- Finding elements by tag name\n",
                "- Extracting text from elements"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "from bs4 import BeautifulSoup\n",
                "\n",
                "# Fetch the webpage\n",
                "url = \"https://bonbanh.com/oto/page,1?q=\"\n",
                "print(f\"Fetching: {url}\\n\")\n",
                "\n",
                "response = requests.get(url)\n",
                "html_content = response.content  # Raw HTML bytes\n",
                "\n",
                "# Parse HTML with BeautifulSoup\n",
                "# 'html.parser' is Python's built-in parser (no extra install needed)\n",
                "soup = BeautifulSoup(html_content, 'html.parser')\n",
                "\n",
                "print(\"=\" * 50)\n",
                "print(\"EXTRACTING CAR TITLES\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Find all <h3> tags which contain the car titles\n",
                "# On Bonbanh, each car listing has a <h3> with the title\n",
                "h3_elements = soup.find_all('h3')\n",
                "\n",
                "print(f\"Found {len(h3_elements)} car listings\\n\")\n",
                "\n",
                "# Extract and print the text from each title\n",
                "for i, h3_element in enumerate(h3_elements, 1):\n",
                "    title_text = h3_element.get_text(strip=True)  # strip=True removes extra whitespace\n",
                "    print(f\"{i:2d}. {title_text}\")\n",
                "\n",
                "print(\"\\n‚úÖ Successfully extracted car titles!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### üí° Key Takeaways\n",
                "\n",
                "- `BeautifulSoup(html, 'html.parser')` creates a parse tree\n",
                "- `soup.find_all('tag')` finds all elements with that tag\n",
                "- `.get_text(strip=True)` extracts text content"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## Step 3: Extract Multiple Data Fields\n",
                "\n",
                "**Goal**: Extract title, price, URL, and year from car listings\n",
                "\n",
                "**Concepts**:\n",
                "- CSS selectors for different elements\n",
                "- Extracting attributes (href)\n",
                "- Regular expressions for pattern matching\n",
                "- Storing data in dictionaries"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import re\n",
                "\n",
                "# Fetch and parse\n",
                "url = \"https://bonbanh.com/oto/page,2?q=\"\n",
                "response = requests.get(url)\n",
                "soup = BeautifulSoup(response.content, 'html.parser')\n",
                "\n",
                "# Find all list items (li) that contain car listings\n",
                "# On Bonbanh, car listings are in <li> elements with <h3> tags\n",
                "all_lis = soup.find_all('li')\n",
                "car_listings = [li for li in all_lis if li.find('h3')]\n",
                "\n",
                "print(\"=\" * 70)\n",
                "print(\"EXTRACTING STRUCTURED DATA\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "cars = []  # List to store all car data\n",
                "\n",
                "# Process each car listing (limit to first 5 for demo)\n",
                "for container in car_listings[:5]:\n",
                "    # Extract title from H3\n",
                "    h3_elem = container.find('h3')\n",
                "    if not h3_elem:\n",
                "        continue\n",
                "    \n",
                "    title = h3_elem.get_text(strip=True)\n",
                "    \n",
                "    # Extract URL from link in H3\n",
                "    link = h3_elem.find('a')\n",
                "    url_raw = link.get('href', '') if link else ''\n",
                "    full_url = f\"https://bonbanh.com{url_raw}\" if url_raw and not url_raw.startswith('http') else url_raw\n",
                "    \n",
                "    # Extract price (look for price class or text)\n",
                "    price_elem = container.find('div', class_='price') or container.find('span', class_='price')\n",
                "    price = price_elem.get_text(strip=True) if price_elem else \"Contact\"\n",
                "    \n",
                "    # Extract year using regex from title\n",
                "    year = 0\n",
                "    year_match = re.search(r'\\b(20\\d{2}|19\\d{2})\\b', title)\n",
                "    if year_match:\n",
                "        year = int(year_match.group(1))\n",
                "    \n",
                "    # Store in dictionary\n",
                "    car_data = {\n",
                "        \"title\": title,\n",
                "        \"price\": price,\n",
                "        \"url\": full_url,\n",
                "        \"year\": year\n",
                "    }\n",
                "    \n",
                "    cars.append(car_data)\n",
                "\n",
                "# Print results\n",
                "print(f\"\\nExtracted data for {len(cars)} cars:\\n\")\n",
                "\n",
                "for i, car in enumerate(cars, 1):\n",
                "    print(f\"--- Car {i} ---\")\n",
                "    print(f\"Title: {car['title']}\")\n",
                "    print(f\"Price: {car['price']}\")\n",
                "    print(f\"Year:  {car['year']}\")\n",
                "    print(f\"URL:   {car['url'][:60]}...\")\n",
                "    print()\n",
                "\n",
                "print(\"‚úÖ Successfully extracted structured data!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### üí° Key Takeaways\n",
                "\n",
                "- `element.find('a')` finds child elements\n",
                "- `element.get('href')` extracts attributes\n",
                "- Use regex `re.search()` for pattern matching"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## Step 4: Complete Crawler with Pydantic Validation\n",
                "\n",
                "**Goal**: Add data validation and export to JSON\n",
                "\n",
                "**Concepts**:\n",
                "- Pydantic models for data validation\n",
                "- Type checking\n",
                "- JSON export"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "from pydantic import BaseModel, Field\n",
                "import json\n",
                "\n",
                "# Define Pydantic model\n",
                "class CarListing(BaseModel):\n",
                "    \"\"\"Represents a single car listing from Bonbanh.com\"\"\"\n",
                "    title: str = Field(..., description=\"Car title\")\n",
                "    price: str = Field(..., description=\"Price (as displayed)\")\n",
                "    url: str = Field(..., description=\"Full URL to listing\")\n",
                "    year: int = Field(default=0, description=\"Year of manufacture\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "def fetch_car_listings(page=1):\n",
                "    \"\"\"Fetch and parse car listings from Bonbanh\"\"\"\n",
                "    url = f\"https://bonbanh.com/oto/page,{page}?q=\"\n",
                "    print(f\"Fetching page {page}...\")\n",
                "    \n",
                "    response = requests.get(url)\n",
                "    soup = BeautifulSoup(response.content, 'html.parser')\n",
                "    \n",
                "    # Find all LI elements with H3 (car listings)\n",
                "    all_lis = soup.find_all('li')\n",
                "    car_listings = [li for li in all_lis if li.find('h3')]\n",
                "    listings = []\n",
                "    \n",
                "    for container in car_listings[:10]:  # Limit to 10 for demo\n",
                "        try:\n",
                "            # Extract title from H3\n",
                "            h3_elem = container.find('h3')\n",
                "            if not h3_elem:\n",
                "                continue\n",
                "            \n",
                "            title = h3_elem.get_text(strip=True)\n",
                "            \n",
                "            # Extract URL\n",
                "            link = h3_elem.find('a')\n",
                "            url_raw = link.get('href', '') if link else ''\n",
                "            full_url = f\"https://bonbanh.com{url_raw}\" if url_raw and not url_raw.startswith('http') else url_raw\n",
                "            \n",
                "            # Extract price\n",
                "            price_elem = container.find('div', class_='price') or container.find('span', class_='price')\n",
                "            price = price_elem.get_text(strip=True) if price_elem else \"Contact\"\n",
                "            \n",
                "            year = 0\n",
                "            year_match = re.search(r'\\b(20\\d{2}|19\\d{2})\\b', title)\n",
                "            if year_match:\n",
                "                year = int(year_match.group(1))\n",
                "            \n",
                "            # Create Pydantic model (validates automatically!)\n",
                "            car = CarListing(\n",
                "                title=title,\n",
                "                price=price,\n",
                "                url=full_url,\n",
                "                year=year\n",
                "            )\n",
                "            \n",
                "            listings.append(car)\n",
                "            \n",
                "        except Exception as e:\n",
                "            # Skip invalid listings\n",
                "            print(f\"Skipped listing due to error: {e}\")\n",
                "            continue\n",
                "    \n",
                "    return listings"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "print(\"=\" * 70)\n",
                "print(\"COMPLETE WEB SCRAPER WITH PYDANTIC VALIDATION\")\n",
                "print(\"=\" * 70)\n",
                "print()\n",
                "\n",
                "# Scrape data\n",
                "cars = fetch_car_listings(page=1)\n",
                "\n",
                "print(f\"\\n‚úÖ Successfully scraped {len(cars)} car listings\")\n",
                "\n",
                "# Convert to JSON\n",
                "car_dicts = [car.model_dump() for car in cars]\n",
                "\n",
                "# Print sample\n",
                "if cars:\n",
                "    print(\"\\n\" + \"=\" * 70)\n",
                "    print(\"SAMPLE OUTPUT:\")\n",
                "    print(\"=\" * 70)\n",
                "    print(cars[0].model_dump_json(indent=2))\n",
                "\n",
                "print(\"\\nüéâ You've built a working web scraper!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## üèÜ Exercises\n",
                "\n",
                "Try these challenges:\n",
                "\n",
                "1. **Add more fields**: Modify `CarListing` to include `location`, `mileage`, `fuel_type`\n",
                "2. **Multi-page**: Modify `fetch_car_listings` to crawl pages 1-5\n",
                "3. **Save to JSON file**: Write the results to `car_listings.json`\n",
                "4. **Filter**: Only include cars newer than 2015"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# Exercise: Save to JSON file\n",
                "output_file = \"car_listings.json\"\n",
                "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
                "    json.dump(car_dicts, f, ensure_ascii=False, indent=2)\n",
                "\n",
                "print(f\"‚úÖ Saved to {output_file}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        }
    ]
}