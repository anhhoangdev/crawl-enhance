{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üï∑Ô∏è Module 1: Static HTML Scraping\n",
                "\n",
                "### *\"The internet is just text. Let's read it.\"*\n",
                "\n",
                "---\n",
                "\n",
                "**Yo, welcome to web scraping!** üëã\n",
                "\n",
                "Before we dive deep into fancy stuff, let's get the fundamentals right. Today we're gonna learn how to:\n",
                "\n",
                "1. **Fetch HTML** ‚Üí basically asking a website \"hey, can I see your code?\"\n",
                "2. **Parse it** ‚Üí making sense of that messy HTML soup\n",
                "3. **Extract data** ‚Üí getting the good stuff we actually need\n",
                "4. **Validate** ‚Üí making sure our data isn't garbage\n",
                "\n",
                "> üí≠ *\"The best scrapers are lazy scrapers. We write code so we don't have to copy-paste.\"*\n",
                "\n",
                "**Target**: [Bonbanh.com](https://bonbanh.com) - a Vietnamese car marketplace"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîß Setup\n",
                "\n",
                "Run this once. Don't overthink it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install requests beautifulsoup4 pydantic -q"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 1: The HTTP Request\n",
                "\n",
                "### What's actually happening?\n",
                "\n",
                "When you type a URL in your browser, you're basically saying:\n",
                "\n",
                "> *\"Hey server, give me that page.\"*\n",
                "\n",
                "That's a **GET request**. The server responds with HTML. That's it. No magic. ü™Ñ\n",
                "\n",
                "Let's do the same thing, but with Python."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "\n",
                "# This is our target\n",
                "url = \"https://bonbanh.com/oto/page,1?q=\"\n",
                "\n",
                "# Send the request (just like your browser does)\n",
                "response = requests.get(url)\n",
                "\n",
                "# Did it work?\n",
                "print(f\"Status: {response.status_code}\")  # 200 = success, 404 = not found, 403 = blocked\n",
                "print(f\"Content-Type: {response.headers.get('content-type')}\")\n",
                "print(f\"\\nHTML size: {len(response.text):,} characters\")\n",
                "\n",
                "# Let's peek at what we got\n",
                "print(\"\\n\" + \"‚îÄ\" * 50)\n",
                "print(\"First 300 chars:\")\n",
                "print(\"‚îÄ\" * 50)\n",
                "print(response.text[:300])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üß† What you should notice:\n",
                "\n",
                "| Code | Meaning |\n",
                "|------|---------|\n",
                "| `200` | All good, we got the page |\n",
                "| `403` | Access denied (we might be blocked) |\n",
                "| `404` | Page doesn't exist |\n",
                "| `503` | Server is having a bad day |\n",
                "\n",
                "> **Pro tip**: If you're getting 403s, the website might be blocking automated requests. We'll handle that in Module 3."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 2: Parsing HTML\n",
                "\n",
                "### The problem with raw HTML\n",
                "\n",
                "That HTML we just got? It's a mess. It's like trying to read a book where all the pages are shuffled.\n",
                "\n",
                "**BeautifulSoup** helps us navigate through this chaos. It turns that string into a tree structure we can actually work with.\n",
                "\n",
                "Think of it like this:\n",
                "```\n",
                "Raw HTML  ‚Üí  BeautifulSoup  ‚Üí  Organized Tree\n",
                "(chaos)        (parser)         (makes sense)\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from bs4 import BeautifulSoup\n",
                "\n",
                "# Fetch fresh data\n",
                "response = requests.get(\"https://bonbanh.com/oto/page,1?q=\")\n",
                "\n",
                "# Parse it\n",
                "soup = BeautifulSoup(response.content, 'html.parser')\n",
                "\n",
                "# Now let's find all car titles\n",
                "# On Bonbanh, each car listing has an <h3> tag with the title\n",
                "titles = soup.find_all('h3')\n",
                "\n",
                "print(f\"Found {len(titles)} car listings\\n\")\n",
                "print(\"‚îÄ\" * 50)\n",
                "\n",
                "# Print the first 10\n",
                "for i, h3 in enumerate(titles[:10], 1):\n",
                "    print(f\"{i:2}. {h3.get_text(strip=True)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üß† Key methods you'll use constantly:\n",
                "\n",
                "```python\n",
                "soup.find('tag')        # Get first match\n",
                "soup.find_all('tag')    # Get all matches\n",
                "element.get_text()      # Get the text inside\n",
                "element.get('href')     # Get an attribute\n",
                "element.find('child')   # Find inside an element\n",
                "```\n",
                "\n",
                "> **Real talk**: 80% of web scraping is just `find()` and `find_all()`. Master these two."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 3: Extracting Structured Data\n",
                "\n",
                "### From chaos to clarity\n",
                "\n",
                "Titles are cool, but we want more. We want:\n",
                "- **Title** (what car is this?)\n",
                "- **Price** (how much?)\n",
                "- **URL** (link to the listing)\n",
                "- **Year** (when was it made?)\n",
                "\n",
                "This is where it gets interesting."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re  # for regex (pattern matching)\n",
                "\n",
                "# Fetch and parse\n",
                "response = requests.get(\"https://bonbanh.com/oto/page,2?q=\")\n",
                "soup = BeautifulSoup(response.content, 'html.parser')\n",
                "\n",
                "# Find all <li> elements that contain car listings\n",
                "# (they have an <h3> inside them)\n",
                "all_items = soup.find_all('li')\n",
                "car_items = [li for li in all_items if li.find('h3')]\n",
                "\n",
                "print(f\"Found {len(car_items)} car listings\\n\")\n",
                "\n",
                "cars = []\n",
                "for item in car_items[:5]:  # just first 5 for demo\n",
                "    h3 = item.find('h3')\n",
                "    if not h3:\n",
                "        continue\n",
                "    \n",
                "    # Extract title\n",
                "    title = h3.get_text(strip=True)\n",
                "    \n",
                "    # Extract URL from the <a> tag inside <h3>\n",
                "    link = h3.find('a')\n",
                "    url = f\"https://bonbanh.com{link.get('href', '')}\" if link else \"\"\n",
                "    \n",
                "    # Extract price (look for price class)\n",
                "    price_el = item.find('div', class_='price') or item.find('span', class_='price')\n",
                "    price = price_el.get_text(strip=True) if price_el else \"Li√™n h·ªá\"\n",
                "    \n",
                "    # Extract year using regex\n",
                "    # Pattern: find a 4-digit year starting with 19 or 20\n",
                "    year_match = re.search(r'\\b(19|20)\\d{2}\\b', title)\n",
                "    year = int(year_match.group()) if year_match else 0\n",
                "    \n",
                "    cars.append({\n",
                "        \"title\": title,\n",
                "        \"price\": price,\n",
                "        \"url\": url,\n",
                "        \"year\": year\n",
                "    })\n",
                "\n",
                "# Display results\n",
                "for i, car in enumerate(cars, 1):\n",
                "    print(f\"üöó Car {i}\")\n",
                "    print(f\"   Title: {car['title'][:50]}...\")\n",
                "    print(f\"   Price: {car['price']}\")\n",
                "    print(f\"   Year:  {car['year']}\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üß† The regex pattern explained:\n",
                "\n",
                "```python\n",
                "r'\\b(19|20)\\d{2}\\b'\n",
                "```\n",
                "\n",
                "| Part | Meaning |\n",
                "|------|---------|\n",
                "| `\\b` | Word boundary (so \"12020\" doesn't match) |\n",
                "| `(19\\|20)` | Starts with 19 or 20 |\n",
                "| `\\d{2}` | Followed by 2 digits |\n",
                "| `\\b` | Another word boundary |\n",
                "\n",
                "> **Exercise**: What years would this match? What about \"Toyota 2025\"?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 4: Data Validation with Pydantic\n",
                "\n",
                "### Why bother?\n",
                "\n",
                "Real-world data is messy. You'll get:\n",
                "- Missing fields\n",
                "- Wrong types (\"2024\" as string instead of int)\n",
                "- Unexpected values\n",
                "\n",
                "**Pydantic** catches these issues early. It's like a security guard for your data.\n",
                "\n",
                "> *\"Trust no data. Validate everything.\"*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pydantic import BaseModel, Field\n",
                "import json\n",
                "\n",
                "class CarListing(BaseModel):\n",
                "    \"\"\"A validated car listing.\"\"\"\n",
                "    title: str = Field(..., min_length=1)\n",
                "    price: str\n",
                "    url: str\n",
                "    year: int = Field(default=0, ge=0, le=2030)  # must be reasonable\n",
                "\n",
                "# Test it\n",
                "car = CarListing(\n",
                "    title=\"Honda Civic 2020\",\n",
                "    price=\"500 Tri·ªáu\",\n",
                "    url=\"https://bonbanh.com/xe-honda-civic.html\",\n",
                "    year=2020\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Valid car:\")\n",
                "print(car.model_dump_json(indent=2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Now let's use it in our scraper\n",
                "def scrape_bonbanh(page=1):\n",
                "    \"\"\"Scrape car listings from Bonbanh with validation.\"\"\"\n",
                "    url = f\"https://bonbanh.com/oto/page,{page}?q=\"\n",
                "    response = requests.get(url)\n",
                "    soup = BeautifulSoup(response.content, 'html.parser')\n",
                "    \n",
                "    all_items = soup.find_all('li')\n",
                "    car_items = [li for li in all_items if li.find('h3')]\n",
                "    \n",
                "    results = []\n",
                "    for item in car_items[:10]:  # limit for demo\n",
                "        try:\n",
                "            h3 = item.find('h3')\n",
                "            if not h3:\n",
                "                continue\n",
                "            \n",
                "            title = h3.get_text(strip=True)\n",
                "            link = h3.find('a')\n",
                "            url = f\"https://bonbanh.com{link.get('href', '')}\" if link else \"\"\n",
                "            \n",
                "            price_el = item.find('div', class_='price') or item.find('span', class_='price')\n",
                "            price = price_el.get_text(strip=True) if price_el else \"Li√™n h·ªá\"\n",
                "            \n",
                "            year_match = re.search(r'\\b(19|20)\\d{2}\\b', title)\n",
                "            year = int(year_match.group()) if year_match else 0\n",
                "            \n",
                "            # Validate with Pydantic\n",
                "            car = CarListing(title=title, price=price, url=url, year=year)\n",
                "            results.append(car)\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"‚ö†Ô∏è Skipped invalid listing: {e}\")\n",
                "            continue\n",
                "    \n",
                "    return results\n",
                "\n",
                "# Run it\n",
                "cars = scrape_bonbanh(page=1)\n",
                "print(f\"\\n‚úÖ Scraped {len(cars)} valid listings\")\n",
                "\n",
                "if cars:\n",
                "    print(\"\\nSample:\")\n",
                "    print(cars[0].model_dump_json(indent=2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üéØ Save Your Data\n",
                "\n",
                "Always save your scraped data. You never know when you'll need it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save to JSON\n",
                "output = [car.model_dump() for car in cars]\n",
                "\n",
                "with open(\"car_listings.json\", \"w\", encoding=\"utf-8\") as f:\n",
                "    json.dump(output, f, ensure_ascii=False, indent=2)\n",
                "\n",
                "print(\"üíæ Saved to car_listings.json\")\n",
                "\n",
                "# Verify\n",
                "!head -20 car_listings.json"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üèãÔ∏è Practice Time\n",
                "\n",
                "Don't just read. **Do**.\n",
                "\n",
                "### Exercise 1: Add more fields\n",
                "Modify `CarListing` to include:\n",
                "- `location` (where is the car?)\n",
                "- `kilometer` (how many km?)\n",
                "- `fuel_type` (xƒÉng, d·∫ßu, ƒëi·ªán?)\n",
                "\n",
                "### Exercise 2: Multi-page scraping\n",
                "Modify `scrape_bonbanh()` to accept a range of pages:\n",
                "```python\n",
                "def scrape_bonbanh(start_page=1, end_page=5):\n",
                "    ...\n",
                "```\n",
                "\n",
                "### Exercise 3: Filter by year\n",
                "Only keep cars from 2018 or newer.\n",
                "\n",
                "### Exercise 4: Different website\n",
                "Try scraping [xe.chotot.com](https://xe.chotot.com) instead. What's different?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìù Summary\n",
                "\n",
                "| Concept | What you learned |\n",
                "|---------|------------------|\n",
                "| `requests` | Fetch HTML from websites |\n",
                "| `BeautifulSoup` | Parse and navigate HTML |\n",
                "| `find()` / `find_all()` | Locate elements |\n",
                "| `get_text()` / `get()` | Extract data |\n",
                "| `Pydantic` | Validate scraped data |\n",
                "| `json.dump()` | Save data to file |\n",
                "\n",
                "### Next up: Module 2\n",
                "\n",
                "What if the website loads data with JavaScript? `requests` can't see that.\n",
                "\n",
                "We'll need **Selenium** ‚Äì a tool that controls a real browser.\n",
                "\n",
                "*See you there.* ‚úåÔ∏è"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": [],
            "toc_visible": true
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
