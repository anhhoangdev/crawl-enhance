{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 2: Dynamic Content Scraping\n",
                "\n",
                "### *\"When JavaScript enters the chat...\"*\n",
                "\n",
                "---\n",
                "\n",
                "**So here's the thing.** ü§î\n",
                "\n",
                "Some websites don't give you the data in the initial HTML. They:\n",
                "1. Send you a mostly empty page\n",
                "2. Run JavaScript to fetch data\n",
                "3. Render it in your browser\n",
                "\n",
                "`requests` only sees step 1. It's blind to JavaScript.\n",
                "\n",
                "**Solution?** Use a real browser. Automate it. Let it do the JavaScript dance.\n",
                "\n",
                "> üí≠ *\"If you can see it in your browser, you can scrape it.\"*\n",
                "\n",
                "**Target**: [CafeF VNINDEX](https://cafef.vn/du-lieu/Lich-su-giao-dich-vnindex-1.chn) - Stock market data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup\n",
                "\n",
                "Colab already has Chrome. We just need to configure it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!apt-get update -qq\n",
                "!apt-get install -y chromium-chromedriver -qq\n",
                "!pip install selenium pydantic pandas -q\n",
                "\n",
                "print(\"‚úÖ Setup complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from selenium import webdriver\n",
                "from selenium.webdriver.common.by import By\n",
                "from selenium.webdriver.support.ui import WebDriverWait\n",
                "from selenium.webdriver.support import expected_conditions as EC\n",
                "from selenium.webdriver.chrome.options import Options\n",
                "import time\n",
                "\n",
                "def get_driver():\n",
                "    \"\"\"Create a headless Chrome driver for Colab.\"\"\"\n",
                "    options = Options()\n",
                "    options.add_argument('--headless')\n",
                "    options.add_argument('--no-sandbox')\n",
                "    options.add_argument('--disable-dev-shm-usage')\n",
                "    return webdriver.Chrome(options=options)\n",
                "\n",
                "print(\"‚úÖ Driver ready!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 1: Opening a Browser\n",
                "\n",
                "### Your first robot browser\n",
                "\n",
                "Think of Selenium as a puppet master. You tell the browser:\n",
                "- Go here\n",
                "- Click this\n",
                "- Wait for that\n",
                "- Extract data\n",
                "\n",
                "And it just... does it. üé≠"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "driver = get_driver()\n",
                "\n",
                "url = \"https://cafef.vn/du-lieu/Lich-su-giao-dich-vnindex-1.chn\"\n",
                "print(f\"Opening: {url}\")\n",
                "\n",
                "driver.get(url)\n",
                "\n",
                "print(f\"‚úÖ Page loaded!\")\n",
                "print(f\"Title: {driver.title}\")\n",
                "\n",
                "driver.quit()\n",
                "print(\"Browser closed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üß† Basic commands:\n",
                "\n",
                "```python\n",
                "driver.get(url)         # Navigate to URL\n",
                "driver.title            # Get page title\n",
                "driver.page_source      # Get HTML (after JS ran!)\n",
                "driver.quit()           # Close browser\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 2: Waiting for Elements\n",
                "\n",
                "### The #1 mistake beginners make\n",
                "\n",
                "You open a page and immediately try to scrape. But the data isn't there yet!\n",
                "\n",
                "JavaScript is still loading. You need to **wait**.\n",
                "\n",
                "> ‚ö†Ô∏è *\"Never assume data is ready. Always wait for it.\"*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "driver = get_driver()\n",
                "\n",
                "try:\n",
                "    url = \"https://cafef.vn/du-lieu/Lich-su-giao-dich-vnindex-1.chn\"\n",
                "    driver.get(url)\n",
                "    \n",
                "    # Create a waiter (max 20 seconds)\n",
                "    wait = WebDriverWait(driver, 20)\n",
                "    \n",
                "    print(\"Waiting for table to load...\")\n",
                "    \n",
                "    # Wait until this element exists\n",
                "    table_xpath = '//*[@id=\"render-table-owner\"]'\n",
                "    table = wait.until(EC.presence_of_element_located((By.XPATH, table_xpath)))\n",
                "    \n",
                "    print(\"‚úÖ Table found!\")\n",
                "    \n",
                "    # Give JS a bit more time to populate data\n",
                "    time.sleep(2)\n",
                "    \n",
                "    # Count rows\n",
                "    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
                "    print(f\"Rows in table: {len(rows)}\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Error: {e}\")\n",
                "    \n",
                "finally:\n",
                "    driver.quit()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üß† Wait strategies:\n",
                "\n",
                "| Method | When to use |\n",
                "|--------|-------------|\n",
                "| `presence_of_element_located` | Element exists in DOM |\n",
                "| `visibility_of_element_located` | Element is visible to user |\n",
                "| `element_to_be_clickable` | Can be clicked |\n",
                "| `time.sleep(n)` | Last resort, avoid if possible |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 3: Extracting Table Data\n",
                "\n",
                "### XPath ‚Äì Your new best friend\n",
                "\n",
                "XPath is like GPS for HTML. It tells you exactly where an element is.\n",
                "\n",
                "```\n",
                "//*[@id=\"render-table-owner\"]  ‚Üí  \"Find any element with this ID\"\n",
                "//tr                           ‚Üí  \"Find all <tr> elements\"\n",
                "//td[1]                        ‚Üí  \"Find first <td> in each row\"\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "driver = get_driver()\n",
                "\n",
                "try:\n",
                "    driver.get(\"https://cafef.vn/du-lieu/Lich-su-giao-dich-vnindex-1.chn\")\n",
                "    \n",
                "    wait = WebDriverWait(driver, 20)\n",
                "    table = wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"render-table-owner\"]')))\n",
                "    time.sleep(2)\n",
                "    \n",
                "    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
                "    \n",
                "    print(f\"Found {len(rows)} rows\\n\")\n",
                "    print(\"‚îÄ\" * 60)\n",
                "    print(\"First 5 rows:\")\n",
                "    print(\"‚îÄ\" * 60)\n",
                "    \n",
                "    for i, row in enumerate(rows[:5], 1):\n",
                "        cells = row.find_elements(By.TAG_NAME, \"td\")\n",
                "        if not cells:\n",
                "            continue\n",
                "            \n",
                "        data = [c.text.strip() for c in cells]\n",
                "        \n",
                "        # The table structure:\n",
                "        # 0: Date, 1: Close, 9: Open, 10: High, 11: Low\n",
                "        if len(data) > 1:\n",
                "            print(f\"{i}. Date: {data[0]:<12} Close: {data[1]:<10}\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"‚ùå {e}\")\n",
                "    \n",
                "finally:\n",
                "    driver.quit()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 4: Complete Crawler\n",
                "\n",
                "### Putting it all together\n",
                "\n",
                "Now let's build a proper crawler with:\n",
                "- Pydantic validation\n",
                "- Export to CSV\n",
                "- Error handling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pydantic import BaseModel, Field\n",
                "import pandas as pd\n",
                "import json\n",
                "\n",
                "class StockData(BaseModel):\n",
                "    \"\"\"One day of VNINDEX data.\"\"\"\n",
                "    date: str\n",
                "    close_price: float = 0.0\n",
                "    open_price: float = 0.0\n",
                "    high_price: float = 0.0\n",
                "    low_price: float = 0.0\n",
                "\n",
                "def parse_number(text):\n",
                "    \"\"\"Convert Vietnamese number format to float.\"\"\"\n",
                "    if not text or text == '-':\n",
                "        return 0.0\n",
                "    try:\n",
                "        return float(text.replace(',', ''))\n",
                "    except:\n",
                "        return 0.0\n",
                "\n",
                "print(\"‚úÖ Models defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def scrape_vnindex():\n",
                "    \"\"\"Scrape VNINDEX data from CafeF.\"\"\"\n",
                "    driver = get_driver()\n",
                "    results = []\n",
                "    \n",
                "    try:\n",
                "        print(\"üöÄ Starting crawler...\")\n",
                "        driver.get(\"https://cafef.vn/du-lieu/Lich-su-giao-dich-vnindex-1.chn\")\n",
                "        \n",
                "        wait = WebDriverWait(driver, 20)\n",
                "        table = wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"render-table-owner\"]')))\n",
                "        time.sleep(2)\n",
                "        \n",
                "        rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
                "        print(f\"üìä Found {len(rows)} rows\")\n",
                "        \n",
                "        for row in rows:\n",
                "            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
                "            if not cells:\n",
                "                continue\n",
                "            \n",
                "            data = [c.text.strip() for c in cells]\n",
                "            date = data[0] if len(data) > 0 else \"\"\n",
                "            \n",
                "            # Skip headers\n",
                "            if not date or \"Ng√†y\" in date:\n",
                "                continue\n",
                "            \n",
                "            try:\n",
                "                stock = StockData(\n",
                "                    date=date,\n",
                "                    close_price=parse_number(data[1] if len(data) > 1 else \"0\"),\n",
                "                    open_price=parse_number(data[9] if len(data) > 9 else \"0\"),\n",
                "                    high_price=parse_number(data[10] if len(data) > 10 else \"0\"),\n",
                "                    low_price=parse_number(data[11] if len(data) > 11 else \"0\")\n",
                "                )\n",
                "                results.append(stock)\n",
                "            except:\n",
                "                continue\n",
                "        \n",
                "        print(f\"‚úÖ Extracted {len(results)} valid records\")\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error: {e}\")\n",
                "        \n",
                "    finally:\n",
                "        driver.quit()\n",
                "    \n",
                "    return results\n",
                "\n",
                "# Run it\n",
                "data = scrape_vnindex()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if data:\n",
                "    # Convert to DataFrame\n",
                "    df = pd.DataFrame([d.model_dump() for d in data])\n",
                "    \n",
                "    # Save\n",
                "    df.to_csv(\"vnindex_data.csv\", index=False)\n",
                "    print(\"üíæ Saved to vnindex_data.csv\\n\")\n",
                "    \n",
                "    # Preview\n",
                "    print(df.head())\n",
                "else:\n",
                "    print(\"No data to save.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üèãÔ∏è Practice Time\n",
                "\n",
                "### Exercise 1: Handle pagination\n",
                "CafeF has multiple pages. Modify the scraper to click \"Next\" and collect more data.\n",
                "\n",
                "### Exercise 2: Different index\n",
                "Change the URL to scrape HNX instead of VNINDEX.\n",
                "\n",
                "### Exercise 3: Visualize\n",
                "Plot the closing prices using matplotlib.\n",
                "\n",
                "### Exercise 4: Compare speed\n",
                "Time how long this takes vs Module 3's async approach."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìù Summary\n",
                "\n",
                "| Concept | What you learned |\n",
                "|---------|------------------|\n",
                "| Selenium | Control a real browser |\n",
                "| WebDriverWait | Wait for elements to load |\n",
                "| XPath | Navigate to specific elements |\n",
                "| Headless mode | Run without visible browser |\n",
                "\n",
                "### ‚ö†Ô∏è The catch\n",
                "\n",
                "Selenium is **slow**. Loading a browser, rendering JavaScript, waiting for elements...\n",
                "\n",
                "For 200 records, it might take 30-60 seconds.\n",
                "\n",
                "### Next up: Module 3\n",
                "\n",
                "What if we could skip all that and call the API directly?\n",
                "\n",
                "**Spoiler**: We can. And it's 5-10x faster. ‚ö°\n",
                "\n",
                "*See you in Module 3.* ‚úåÔ∏è"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": [],
            "toc_visible": true
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
