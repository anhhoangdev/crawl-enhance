{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "toc_visible": true
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# üìö Module 3: API-Based Scraping (Async)\n",
                "\n",
                "**Learn high-performance data collection via APIs**\n",
                "\n",
                "In this notebook, you'll learn:\n",
                "- How to find hidden APIs using browser DevTools\n",
                "- Direct API calls (no browser needed!)\n",
                "- Async programming with `aiohttp`\n",
                "- Rate limiting and best practices\n",
                "\n",
                "**Target**: CafeF Stock API\n",
                "\n",
                "‚ö° **This is 5-10x faster than Selenium!**"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üîß Setup"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "!pip install aiohttp pydantic pandas nest_asyncio -q\n",
                "\n",
                "# Enable async in Jupyter/Colab\n",
                "import nest_asyncio\n",
                "nest_asyncio.apply()\n",
                "\n",
                "print(\"‚úÖ Setup complete!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## Step 1: How to Find API Endpoints\n",
                "\n",
                "This is a **guide** on how to discover hidden APIs!\n",
                "\n",
                "### üîç Step-by-Step:\n",
                "\n",
                "1. **Open the Website**: Navigate to [CafeF VNINDEX](https://cafef.vn/du-lieu/Lich-su-giao-dich-vnindex-1.chn)\n",
                "\n",
                "2. **Open DevTools**: Press `F12` ‚Üí Click \"Network\" tab\n",
                "\n",
                "3. **Filter for XHR**: Click \"Fetch/XHR\" to show only API calls\n",
                "\n",
                "4. **Reload the Page**: Press `F5` and watch requests appear\n",
                "\n",
                "5. **Find the API**: Look for `PriceHistory.ashx` üéØ\n",
                "\n",
                "6. **Check Response**: Click it ‚Üí \"Response\" tab ‚Üí You'll see JSON!\n",
                "\n",
                "### üìã API Details We Found:\n",
                "\n",
                "```\n",
                "URL: https://cafef.vn/du-lieu/Ajax/PageNew/DataHistory/PriceHistory.ashx\n",
                "Parameters:\n",
                "  - Symbol=VNINDEX\n",
                "  - PageIndex=1\n",
                "  - PageSize=20\n",
                "```\n",
                "\n",
                "### üí° Key Insight:\n",
                "- Many websites load data via APIs\n",
                "- APIs are MUCH faster than scraping HTML\n",
                "- Network tab is your best friend!"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## Step 2: Make a Single API Request\n",
                "\n",
                "**Goal**: Fetch data directly from the API (no browser needed!)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import requests\n",
                "import json\n",
                "\n",
                "print(\"=\" * 70)\n",
                "print(\"CALLING CAFEF API DIRECTLY\")\n",
                "print(\"=\" * 70)\n",
                "print()\n",
                "\n",
                "# The API endpoint we discovered\n",
                "api_url = \"https://cafef.vn/du-lieu/Ajax/PageNew/DataHistory/PriceHistory.ashx\"\n",
                "\n",
                "# Parameters for the request\n",
                "params = {\n",
                "    \"Symbol\": \"VNINDEX\",\n",
                "    \"StartDate\": \"\",\n",
                "    \"EndDate\": \"\",\n",
                "    \"PageIndex\": 1,\n",
                "    \"PageSize\": 20\n",
                "}\n",
                "\n",
                "print(\"API URL:\", api_url)\n",
                "print(\"Parameters:\", json.dumps(params, indent=2))\n",
                "print()\n",
                "\n",
                "# Make the request\n",
                "print(\"Sending request...\")\n",
                "response = requests.get(api_url, params=params)\n",
                "\n",
                "print(f\"Status Code: {response.status_code}\")\n",
                "print()\n",
                "\n",
                "# Parse JSON response\n",
                "data = response.json()\n",
                "\n",
                "if data.get(\"Success\"):\n",
                "    print(\"‚úÖ API call successful!\")\n",
                "    \n",
                "    records = data.get(\"Data\", {}).get(\"Data\", [])\n",
                "    print(f\"Received: {len(records)} records\")\n",
                "    print()\n",
                "    \n",
                "    if records:\n",
                "        print(\"=\" * 70)\n",
                "        print(\"FIRST RECORD:\")\n",
                "        print(\"=\" * 70)\n",
                "        print(json.dumps(records[0], indent=2, ensure_ascii=False))\n",
                "        print()\n",
                "        \n",
                "        print(\"=\" * 70)\n",
                "        print(\"AVAILABLE FIELDS:\")\n",
                "        print(\"=\" * 70)\n",
                "        for key in records[0].keys():\n",
                "            print(f\"  - {key}\")\n",
                "else:\n",
                "    print(\"‚ùå API call failed!\")\n",
                "\n",
                "print(\"\\n‚ö° This is MUCH faster than Selenium!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### üí° Key Takeaways\n",
                "\n",
                "- `requests.get(url, params={...})` sends API request with parameters\n",
                "- `response.json()` parses JSON response\n",
                "- No browser = instant results!"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## Step 3: Async Requests with aiohttp\n",
                "\n",
                "**Goal**: Fetch multiple pages concurrently (parallel requests)\n",
                "\n",
                "**Concepts**:\n",
                "- Asynchronous programming (async/await)\n",
                "- aiohttp for async HTTP\n",
                "- asyncio.gather for parallel execution"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import aiohttp\n",
                "import asyncio\n",
                "from typing import List, Dict\n",
                "import time\n",
                "\n",
                "API_URL = \"https://cafef.vn/du-lieu/Ajax/PageNew/DataHistory/PriceHistory.ashx\"\n",
                "SYMBOL = \"VNINDEX\"\n",
                "\n",
                "async def fetch_page(session: aiohttp.ClientSession, page_index: int) -> List[Dict]:\n",
                "    \"\"\"Fetch a single page asynchronously\"\"\"\n",
                "    params = {\n",
                "        \"Symbol\": SYMBOL,\n",
                "        \"StartDate\": \"\",\n",
                "        \"EndDate\": \"\",\n",
                "        \"PageIndex\": page_index,\n",
                "        \"PageSize\": 20\n",
                "    }\n",
                "    \n",
                "    print(f\"  Fetching page {page_index}...\")\n",
                "    \n",
                "    try:\n",
                "        async with session.get(API_URL, params=params) as response:\n",
                "            data = await response.json(content_type=None)\n",
                "            \n",
                "            if data.get(\"Success\"):\n",
                "                records = data.get(\"Data\", {}).get(\"Data\", [])\n",
                "                print(f\"  ‚úÖ Page {page_index}: {len(records)} records\")\n",
                "                return records\n",
                "            else:\n",
                "                print(f\"  ‚ùå Page {page_index}: Failed\")\n",
                "                return []\n",
                "                \n",
                "    except Exception as e:\n",
                "        print(f\"  ‚ùå Page {page_index}: {e}\")\n",
                "        return []\n",
                "\n",
                "async def fetch_all_pages(total_pages: int = 5) -> List[Dict]:\n",
                "    \"\"\"Fetch multiple pages concurrently\"\"\"\n",
                "    print(\"=\" * 70)\n",
                "    print(f\"FETCHING {total_pages} PAGES CONCURRENTLY\")\n",
                "    print(\"=\" * 70)\n",
                "    print()\n",
                "    \n",
                "    async with aiohttp.ClientSession() as session:\n",
                "        # Create tasks for all pages\n",
                "        tasks = [fetch_page(session, i) for i in range(1, total_pages + 1)]\n",
                "        \n",
                "        # Execute all tasks concurrently!\n",
                "        results = await asyncio.gather(*tasks)\n",
                "        \n",
                "        # Flatten the list\n",
                "        all_records = [record for page_records in results for record in page_records]\n",
                "        \n",
                "        return all_records"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Run the async function\n",
                "start_time = time.time()\n",
                "\n",
                "records = asyncio.run(fetch_all_pages(total_pages=5))\n",
                "\n",
                "elapsed = time.time() - start_time\n",
                "\n",
                "print()\n",
                "print(\"=\" * 70)\n",
                "print(\"RESULTS\")\n",
                "print(\"=\" * 70)\n",
                "print(f\"Total records: {len(records)}\")\n",
                "print(f\"Time taken: {elapsed:.2f} seconds\")\n",
                "print(f\"Speed: {len(records) / elapsed:.1f} records/second\")\n",
                "print()\n",
                "print(\"‚ö° ASYNC IS FAST!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### üí° Key Takeaways\n",
                "\n",
                "- `async def` defines asynchronous functions\n",
                "- `await` waits for async operations\n",
                "- `asyncio.gather(*tasks)` runs tasks in parallel\n",
                "- Much faster than sequential requests!"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## Step 4: Complete Scraper with Rate Limiting\n",
                "\n",
                "**Goal**: Production-ready async scraper\n",
                "\n",
                "**Concepts**:\n",
                "- Semaphore for concurrency control\n",
                "- Rate limiting (be nice to servers!)\n",
                "- Pydantic validation\n",
                "- Export to JSON/CSV"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "from pydantic import BaseModel, Field, field_validator\n",
                "import pandas as pd\n",
                "\n",
                "class StockData(BaseModel):\n",
                "    \"\"\"Validated stock data model\"\"\"\n",
                "    date: str = Field(alias=\"Ngay\")\n",
                "    close_price: float = Field(alias=\"GiaDongCua\")\n",
                "    open_price: float = Field(default=0.0, alias=\"GiaMoCua\")\n",
                "    high_price: float = Field(default=0.0, alias=\"GiaCaoNhat\")\n",
                "    low_price: float = Field(default=0.0, alias=\"GiaThapNhat\")\n",
                "    volume: float = Field(default=0.0, alias=\"KhoiLuong\")\n",
                "    \n",
                "    class Config:\n",
                "        populate_by_name = True\n",
                "\n",
                "print(\"‚úÖ Model defined!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "class CafeFAsyncCrawler:\n",
                "    \"\"\"Production-ready async crawler with rate limiting\"\"\"\n",
                "    \n",
                "    API_URL = \"https://cafef.vn/du-lieu/Ajax/PageNew/DataHistory/PriceHistory.ashx\"\n",
                "    \n",
                "    def __init__(self, symbol: str = \"VNINDEX\", max_concurrent: int = 5):\n",
                "        self.symbol = symbol\n",
                "        self.semaphore = asyncio.Semaphore(max_concurrent)  # Rate limiting!\n",
                "        self.headers = {\n",
                "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
                "        }\n",
                "    \n",
                "    async def fetch_page(self, session: aiohttp.ClientSession, page_index: int) -> List:\n",
                "        \"\"\"Fetch a single page with rate limiting\"\"\"\n",
                "        async with self.semaphore:  # Only N concurrent requests\n",
                "            params = {\n",
                "                \"Symbol\": self.symbol,\n",
                "                \"StartDate\": \"\",\n",
                "                \"EndDate\": \"\",\n",
                "                \"PageIndex\": page_index,\n",
                "                \"PageSize\": 20\n",
                "            }\n",
                "            \n",
                "            try:\n",
                "                async with session.get(self.API_URL, params=params, headers=self.headers) as response:\n",
                "                    data = await response.json(content_type=None)\n",
                "                    \n",
                "                    if not data.get(\"Success\"):\n",
                "                        return []\n",
                "                    \n",
                "                    records = data.get(\"Data\", {}).get(\"Data\", [])\n",
                "                    \n",
                "                    # Validate with Pydantic\n",
                "                    validated = []\n",
                "                    for record in records:\n",
                "                        try:\n",
                "                            stock_data = StockData(**record)\n",
                "                            validated.append(stock_data)\n",
                "                        except:\n",
                "                            continue\n",
                "                    \n",
                "                    print(f\"  ‚úÖ Page {page_index}: {len(validated)} records\")\n",
                "                    return validated\n",
                "                    \n",
                "            except Exception as e:\n",
                "                print(f\"  ‚ùå Page {page_index}: {e}\")\n",
                "                return []\n",
                "    \n",
                "    async def crawl(self, total_pages: int = 10) -> List:\n",
                "        \"\"\"Crawl multiple pages with rate limiting\"\"\"\n",
                "        print(\"=\" * 70)\n",
                "        print(f\"CAFEF ASYNC CRAWLER - {self.symbol}\")\n",
                "        print(f\"Fetching {total_pages} pages (max 5 concurrent)\")\n",
                "        print(\"=\" * 70)\n",
                "        print()\n",
                "        \n",
                "        async with aiohttp.ClientSession() as session:\n",
                "            tasks = [self.fetch_page(session, i) for i in range(1, total_pages + 1)]\n",
                "            results = await asyncio.gather(*tasks)\n",
                "            return [item for page in results for item in page]"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Run the production crawler\n",
                "start_time = time.time()\n",
                "\n",
                "crawler = CafeFAsyncCrawler(symbol=\"VNINDEX\", max_concurrent=5)\n",
                "data = asyncio.run(crawler.crawl(total_pages=10))\n",
                "\n",
                "elapsed = time.time() - start_time\n",
                "\n",
                "print()\n",
                "print(\"=\" * 70)\n",
                "print(\"CRAWLING COMPLETE\")\n",
                "print(\"=\" * 70)\n",
                "print(f\"Total records: {len(data)}\")\n",
                "print(f\"Time taken: {elapsed:.2f} seconds\")\n",
                "print(f\"Speed: {len(data) / elapsed:.1f} records/second\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Export to DataFrame and files\n",
                "if data:\n",
                "    df = pd.DataFrame([d.model_dump() for d in data])\n",
                "    \n",
                "    # Save to CSV\n",
                "    df.to_csv(\"vnindex_final.csv\", index=False)\n",
                "    print(\"üíæ Saved: vnindex_final.csv\")\n",
                "    \n",
                "    # Save to JSON\n",
                "    with open(\"vnindex_final.json\", \"w\") as f:\n",
                "        json.dump([d.model_dump() for d in data], f, indent=2)\n",
                "    print(\"üíæ Saved: vnindex_final.json\")\n",
                "    \n",
                "    print(\"\\n\" + \"=\" * 70)\n",
                "    print(\"DATA PREVIEW:\")\n",
                "    print(\"=\" * 70)\n",
                "    print(df.head())\n",
                "    \n",
                "    print(\"\\nüéâ You've built a production-ready async scraper!\")\n",
                "    print(\"\\n‚ö° SPEED COMPARISON:\")\n",
                "    print(\"  Selenium: ~30-60s for 200 records\")\n",
                "    print(\"  Async API: ~5-10s for 200 records\")\n",
                "    print(\"  That's 5-6x faster!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## üèÜ Exercises\n",
                "\n",
                "1. **Different stock**: Change symbol to \"VN30\" or \"HNX\"\n",
                "2. **More pages**: Increase `total_pages` to 50\n",
                "3. **Date filter**: Add StartDate and EndDate parameters\n",
                "4. **Data analysis**: Calculate average, min, max prices\n",
                "5. **Visualization**: Plot the data with matplotlib"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# Exercise: Visualize the data\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "if 'df' in dir() and len(df) > 0:\n",
                "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
                "    \n",
                "    # Price chart\n",
                "    ax1.plot(df['close_price'].head(50), marker='o', markersize=3)\n",
                "    ax1.set_title('VNINDEX Close Price')\n",
                "    ax1.set_ylabel('Price')\n",
                "    ax1.grid(True)\n",
                "    \n",
                "    # Volume chart\n",
                "    ax2.bar(range(len(df.head(50))), df['volume'].head(50))\n",
                "    ax2.set_title('Trading Volume')\n",
                "    ax2.set_xlabel('Day')\n",
                "    ax2.set_ylabel('Volume')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## üìä Summary: Scraping Techniques Comparison\n",
                "\n",
                "| Technique | Speed | Complexity | When to Use |\n",
                "|-----------|-------|------------|-------------|\n",
                "| **BeautifulSoup** | ‚≠ê‚≠ê‚≠ê | Easy | Static HTML pages |\n",
                "| **Selenium** | ‚≠ê | Medium | JavaScript-rendered content |\n",
                "| **Async API** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Advanced | When API is available |\n",
                "\n",
                "### üéØ Pro Tips:\n",
                "1. Always check for APIs first (fastest option)\n",
                "2. Use rate limiting to avoid being blocked\n",
                "3. Validate data with Pydantic\n",
                "4. Handle errors gracefully"
            ],
            "metadata": {}
        }
    ]
}