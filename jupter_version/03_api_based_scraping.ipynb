{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ‚ö° Module 3: API-Based Scraping\n",
                "\n",
                "### *\"The secret pro technique.\"*\n",
                "\n",
                "---\n",
                "\n",
                "**Plot twist incoming.** üé¨\n",
                "\n",
                "That table you scraped with Selenium? There's probably an API behind it.\n",
                "\n",
                "When you open a webpage, your browser:\n",
                "1. Loads the HTML skeleton\n",
                "2. Runs JavaScript\n",
                "3. JavaScript fetches data from an **API**\n",
                "4. Renders it on screen\n",
                "\n",
                "What if we just... call that API directly?\n",
                "\n",
                "> üí≠ *\"The fastest scraper is one that doesn't scrape at all.\"*\n",
                "\n",
                "**Speed comparison**:\n",
                "- Selenium: ~60s for 200 records\n",
                "- Direct API: ~5s for 200 records\n",
                "\n",
                "That's a **12x performance boost**. Let's go. üöÄ"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîß Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install aiohttp pydantic pandas nest_asyncio -q\n",
                "\n",
                "# Enable async in Jupyter\n",
                "import nest_asyncio\n",
                "nest_asyncio.apply()\n",
                "\n",
                "print(\"‚úÖ Ready!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 1: Finding Hidden APIs\n",
                "\n",
                "### The detective work\n",
                "\n",
                "This is where it gets fun. You're basically reverse-engineering how the website works.\n",
                "\n",
                "**How to find APIs:**\n",
                "\n",
                "1. Open the website in Chrome\n",
                "2. Press `F12` ‚Üí Network tab\n",
                "3. Filter by `Fetch/XHR`\n",
                "4. Reload the page\n",
                "5. Look for URLs that return JSON\n",
                "\n",
                "For CafeF, the magic URL is:\n",
                "```\n",
                "https://cafef.vn/du-lieu/Ajax/PageNew/DataHistory/PriceHistory.ashx\n",
                "  ?Symbol=VNINDEX\n",
                "  &PageIndex=1\n",
                "  &PageSize=20\n",
                "```\n",
                "\n",
                "> üéØ Pro tip: Look for URLs ending in `.json`, `.ashx`, `/api/`, `/Ajax/`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 2: Calling the API Directly\n",
                "\n",
                "### No browser. No JavaScript. Just data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "import json\n",
                "\n",
                "api_url = \"https://cafef.vn/du-lieu/Ajax/PageNew/DataHistory/PriceHistory.ashx\"\n",
                "\n",
                "params = {\n",
                "    \"Symbol\": \"VNINDEX\",\n",
                "    \"StartDate\": \"\",\n",
                "    \"EndDate\": \"\",\n",
                "    \"PageIndex\": 1,\n",
                "    \"PageSize\": 20\n",
                "}\n",
                "\n",
                "print(f\"Calling: {api_url}\")\n",
                "print(f\"Params: {params}\\n\")\n",
                "\n",
                "response = requests.get(api_url, params=params)\n",
                "data = response.json()\n",
                "\n",
                "if data.get(\"Success\"):\n",
                "    records = data[\"Data\"][\"Data\"]\n",
                "    print(f\"‚úÖ Got {len(records)} records\\n\")\n",
                "    \n",
                "    # Show first record\n",
                "    print(\"Sample record:\")\n",
                "    print(json.dumps(records[0], indent=2, ensure_ascii=False))\n",
                "else:\n",
                "    print(\"‚ùå API returned error\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üß† What just happened?\n",
                "\n",
                "We got the same data that Selenium had to:\n",
                "1. Open Chrome\n",
                "2. Load the page\n",
                "3. Wait for JavaScript\n",
                "4. Parse the HTML\n",
                "\n",
                "But we did it in **one line**.\n",
                "\n",
                "> This is the cheat code. üéÆ"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 3: Going Async\n",
                "\n",
                "### Why async?\n",
                "\n",
                "Normal code: Do one thing ‚Üí Wait ‚Üí Do next thing ‚Üí Wait ‚Üí ...\n",
                "\n",
                "Async code: Start 10 things at once ‚Üí Wait for all ‚Üí Done\n",
                "\n",
                "```\n",
                "Sync:   |‚îÄ‚îÄPage1‚îÄ‚îÄ|‚îÄ‚îÄPage2‚îÄ‚îÄ|‚îÄ‚îÄPage3‚îÄ‚îÄ|  (slow)\n",
                "Async:  |‚îÄ‚îÄPage1‚îÄ‚îÄ|                     (fast!)\n",
                "        |‚îÄ‚îÄPage2‚îÄ‚îÄ|\n",
                "        |‚îÄ‚îÄPage3‚îÄ‚îÄ|\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import aiohttp\n",
                "import asyncio\n",
                "import time\n",
                "\n",
                "API_URL = \"https://cafef.vn/du-lieu/Ajax/PageNew/DataHistory/PriceHistory.ashx\"\n",
                "\n",
                "async def fetch_page(session, page):\n",
                "    \"\"\"Fetch one page of data.\"\"\"\n",
                "    params = {\n",
                "        \"Symbol\": \"VNINDEX\",\n",
                "        \"StartDate\": \"\",\n",
                "        \"EndDate\": \"\",\n",
                "        \"PageIndex\": page,\n",
                "        \"PageSize\": 20\n",
                "    }\n",
                "    \n",
                "    async with session.get(API_URL, params=params) as resp:\n",
                "        data = await resp.json(content_type=None)\n",
                "        \n",
                "        if data.get(\"Success\"):\n",
                "            records = data[\"Data\"][\"Data\"]\n",
                "            print(f\"  ‚úÖ Page {page}: {len(records)} records\")\n",
                "            return records\n",
                "        return []\n",
                "\n",
                "async def fetch_all(total_pages=5):\n",
                "    \"\"\"Fetch multiple pages concurrently.\"\"\"\n",
                "    print(f\"Fetching {total_pages} pages in parallel...\\n\")\n",
                "    \n",
                "    async with aiohttp.ClientSession() as session:\n",
                "        tasks = [fetch_page(session, i) for i in range(1, total_pages + 1)]\n",
                "        results = await asyncio.gather(*tasks)\n",
                "        \n",
                "        # Flatten\n",
                "        all_records = [r for page in results for r in page]\n",
                "        return all_records"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Time it!\n",
                "start = time.time()\n",
                "\n",
                "records = asyncio.run(fetch_all(total_pages=5))\n",
                "\n",
                "elapsed = time.time() - start\n",
                "\n",
                "print(f\"\\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\n",
                "print(f\"‚îÇ  Total records: {len(records):>13} ‚îÇ\")\n",
                "print(f\"‚îÇ  Time taken: {elapsed:>13.2f}s ‚îÇ\")\n",
                "print(f\"‚îÇ  Speed: {len(records)/elapsed:>13.1f}/sec ‚îÇ\")\n",
                "print(f\"‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üß† Key concepts:\n",
                "\n",
                "| Concept | What it does |\n",
                "|---------|-------------|\n",
                "| `async def` | Declare an async function |\n",
                "| `await` | Wait for async operation |\n",
                "| `asyncio.gather()` | Run tasks in parallel |\n",
                "| `aiohttp.ClientSession` | Async HTTP client |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 4: Production-Ready Crawler\n",
                "\n",
                "### Adding the important stuff\n",
                "\n",
                "Real-world APIs can:\n",
                "- Rate limit you (too many requests = blocked)\n",
                "- Return bad data\n",
                "- Be temporarily unavailable\n",
                "\n",
                "We need:\n",
                "1. **Rate limiting** ‚Üí Don't spam\n",
                "2. **Validation** ‚Üí Clean data only\n",
                "3. **Error handling** ‚Üí Don't crash"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pydantic import BaseModel, Field\n",
                "import pandas as pd\n",
                "\n",
                "class StockData(BaseModel):\n",
                "    \"\"\"Validated stock data.\"\"\"\n",
                "    date: str = Field(alias=\"Ngay\")\n",
                "    close: float = Field(alias=\"GiaDongCua\")\n",
                "    open: float = Field(default=0.0, alias=\"GiaMoCua\")\n",
                "    high: float = Field(default=0.0, alias=\"GiaCaoNhat\")\n",
                "    low: float = Field(default=0.0, alias=\"GiaThapNhat\")\n",
                "    volume: float = Field(default=0.0, alias=\"KhoiLuong\")\n",
                "    \n",
                "    class Config:\n",
                "        populate_by_name = True\n",
                "\n",
                "print(\"‚úÖ Model defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class VNIndexCrawler:\n",
                "    \"\"\"Production-ready async crawler.\"\"\"\n",
                "    \n",
                "    def __init__(self, symbol=\"VNINDEX\", max_concurrent=5):\n",
                "        self.symbol = symbol\n",
                "        self.semaphore = asyncio.Semaphore(max_concurrent)  # Rate limit!\n",
                "        self.api_url = \"https://cafef.vn/du-lieu/Ajax/PageNew/DataHistory/PriceHistory.ashx\"\n",
                "    \n",
                "    async def fetch_page(self, session, page):\n",
                "        \"\"\"Fetch one page with rate limiting.\"\"\"\n",
                "        async with self.semaphore:  # <-- Only N requests at a time\n",
                "            params = {\n",
                "                \"Symbol\": self.symbol,\n",
                "                \"StartDate\": \"\",\n",
                "                \"EndDate\": \"\",\n",
                "                \"PageIndex\": page,\n",
                "                \"PageSize\": 20\n",
                "            }\n",
                "            \n",
                "            try:\n",
                "                async with session.get(self.api_url, params=params) as resp:\n",
                "                    data = await resp.json(content_type=None)\n",
                "                    \n",
                "                    if not data.get(\"Success\"):\n",
                "                        return []\n",
                "                    \n",
                "                    # Validate each record\n",
                "                    validated = []\n",
                "                    for record in data[\"Data\"][\"Data\"]:\n",
                "                        try:\n",
                "                            validated.append(StockData(**record))\n",
                "                        except:\n",
                "                            continue\n",
                "                    \n",
                "                    print(f\"  ‚úÖ Page {page}: {len(validated)} records\")\n",
                "                    return validated\n",
                "                    \n",
                "            except Exception as e:\n",
                "                print(f\"  ‚ùå Page {page}: {e}\")\n",
                "                return []\n",
                "    \n",
                "    async def crawl(self, pages=10):\n",
                "        \"\"\"Crawl multiple pages.\"\"\"\n",
                "        print(f\"üöÄ Crawling {self.symbol} ({pages} pages, max 5 concurrent)\\n\")\n",
                "        \n",
                "        async with aiohttp.ClientSession() as session:\n",
                "            tasks = [self.fetch_page(session, i) for i in range(1, pages + 1)]\n",
                "            results = await asyncio.gather(*tasks)\n",
                "            return [r for page in results for r in page]\n",
                "\n",
                "print(\"‚úÖ Crawler class defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run it!\n",
                "start = time.time()\n",
                "\n",
                "crawler = VNIndexCrawler(symbol=\"VNINDEX\", max_concurrent=5)\n",
                "data = asyncio.run(crawler.crawl(pages=10))\n",
                "\n",
                "elapsed = time.time() - start\n",
                "\n",
                "print(f\"\\n{'‚ïê'*40}\")\n",
                "print(f\"üìä Results: {len(data)} records in {elapsed:.2f}s\")\n",
                "print(f\"‚ö° Speed: {len(data)/elapsed:.1f} records/sec\")\n",
                "print(f\"{'‚ïê'*40}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save it\n",
                "if data:\n",
                "    df = pd.DataFrame([d.model_dump() for d in data])\n",
                "    \n",
                "    df.to_csv(\"vnindex_api.csv\", index=False)\n",
                "    print(\"üíæ Saved to vnindex_api.csv\\n\")\n",
                "    \n",
                "    print(df.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üèãÔ∏è Practice Time\n",
                "\n",
                "### Exercise 1: Different symbol\n",
                "Try `VN30`, `HNX`, or a specific stock like `VNM`.\n",
                "\n",
                "### Exercise 2: Date range\n",
                "Fill in `StartDate` and `EndDate` parameters.\n",
                "\n",
                "### Exercise 3: Visualize\n",
                "Plot closing prices over time.\n",
                "\n",
                "### Exercise 4: Find another API\n",
                "Pick any website with a table. Use DevTools to find its API."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìù Final Summary\n",
                "\n",
                "### The Scraping Hierarchy\n",
                "\n",
                "| Level | Tool | Speed | When to use |\n",
                "|-------|------|-------|-------------|\n",
                "| 1 | `requests` + BS4 | ‚≠ê‚≠ê‚≠ê | Static HTML |\n",
                "| 2 | Selenium | ‚≠ê | JavaScript pages |\n",
                "| 3 | Direct API | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | When API exists |\n",
                "\n",
                "### The workflow\n",
                "\n",
                "```\n",
                "1. Try to find an API first (DevTools ‚Üí Network ‚Üí XHR)\n",
                "2. If no API, check if data is in static HTML (requests + BS4)\n",
                "3. Last resort: Use Selenium\n",
                "```\n",
                "\n",
                "### What you learned\n",
                "\n",
                "| Module | Concept |\n",
                "|--------|--------_|\n",
                "| 1 | HTTP requests, HTML parsing |\n",
                "| 2 | Browser automation, waits |\n",
                "| 3 | API discovery, async programming |\n",
                "\n",
                "---\n",
                "\n",
                "## üéì You're done!\n",
                "\n",
                "You now know the fundamentals of web scraping at three different levels.\n",
                "\n",
                "The next step? **Build something real.**\n",
                "\n",
                "Some ideas:\n",
                "- Price tracker for e-commerce sites\n",
                "- News aggregator\n",
                "- Job listing collector\n",
                "- Social media dashboard\n",
                "\n",
                "> *\"The best way to learn is to build.\"*\n",
                "\n",
                "Go make something cool. ‚úåÔ∏è"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": [],
            "toc_visible": true
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}